# Tokkenization
This is a sheet of python that how we can divided the words into tokens.(Using nlp)
