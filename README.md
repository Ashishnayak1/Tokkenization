# Tokkenization
This is a sheet of python that how we can divided the words into tokens.(Using nlp)
This is a very basic sheet one can easily get idea that how we can use Nlp from sentence to word processing.
